{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2003ec",
   "metadata": {},
   "source": [
    "# Capstone Project: Model Comparison (GridSearch + Cross-Validation)\n",
    "\n",
    "**Models:** Logistic Regression (baseline) vs XGBoost (boosted trees)\n",
    "\n",
    "**Goal:** Compare models for predicting **30-day readmission** (`readmitted_binary`) using **cross-validation** and **GridSearchCV**, then evaluate best models on a held-out test set.\n",
    "\n",
    "> Designed to run in VS Code. Saves artifacts to `output/readmitted_binary/model_comparison_gridsearch/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports and Setup\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "OUTPUT_DIR = Path(\"../output/readmitted_binary/model_comparison_gridsearch\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Output dir:\", OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6cf97",
   "metadata": {},
   "source": [
    "## 2) Load Data\n",
    "\n",
    "Expected path: `Processed Data/processed_diabetes_data.csv`.\n",
    "\n",
    "This notebook tries:\n",
    "- `../Processed Data/processed_diabetes_data.csv` (if notebook is inside `notebooks/`)\n",
    "- `Processed Data/processed_diabetes_data.csv` (if notebook is in repo root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load Data\n",
    "from pathlib import Path\n",
    "\n",
    "def load_processed_data():\n",
    "    candidates = [\n",
    "        Path(\"../Processed Data/processed_diabetes_data.csv\"),\n",
    "        Path(\"Processed Data/processed_diabetes_data.csv\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            print(f\"Loaded: {p}\")\n",
    "            return pd.read_csv(p)\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find processed_diabetes_data.csv. \"\n",
    "        \"Expected in 'Processed Data/processed_diabetes_data.csv'.\"\n",
    "    )\n",
    "\n",
    "data = load_processed_data()\n",
    "print(\"Shape:\", data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6b0da",
   "metadata": {},
   "source": [
    "## 3) Define Target and Features\n",
    "\n",
    "We use `readmitted_binary` if present; otherwise we fall back to `readmitted`.\n",
    "Target should be binary (0/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28496c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define Target and Features\n",
    "target_candidates = [\"readmitted_binary\", \"readmitted\"]\n",
    "target_col = next((c for c in target_candidates if c in data.columns), None)\n",
    "if target_col is None:\n",
    "    raise ValueError(f\"Target not found. Expected one of: {target_candidates}\")\n",
    "\n",
    "y = data[target_col].copy()\n",
    "\n",
    "# Ensure binary numeric 0/1\n",
    "if y.dtype == \"object\":\n",
    "    y = y.astype(str).str.strip()\n",
    "    mapping = {\"1\": 1, \"0\": 0, \"yes\": 1, \"no\": 0, \"<30\": 1, \">30\": 0, \"NO\": 0}\n",
    "    y = y.map(lambda v: mapping.get(v, v)).astype(int)\n",
    "\n",
    "X = data.drop(columns=[target_col])\n",
    "\n",
    "print(\"Target:\", target_col)\n",
    "print(\"y distribution:\\n\", y.value_counts(normalize=True).round(3))\n",
    "print(\"Feature columns:\", X.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8cf34",
   "metadata": {},
   "source": [
    "## 4) Quick EDA (Checks)\n",
    "\n",
    "Saves class balance plot to output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aebfdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Quick EDA\n",
    "missing_pct = (X.isna().mean().sort_values(ascending=False) * 100).round(2)\n",
    "display(missing_pct.head(10))\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "print(\"Numeric cols:\", len(numeric_cols))\n",
    "print(\"Categorical cols:\", len(categorical_cols))\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Class Balance (0 = not <30, 1 = readmitted <30)\")\n",
    "plt.xlabel(\"Target class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"class_balance.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb071a",
   "metadata": {},
   "source": [
    "## 5) Train/Test Split\n",
    "\n",
    "Hold out 20% as the final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74bcd7",
   "metadata": {},
   "source": [
    "## 6) Preprocessing (Impute + Encode)\n",
    "\n",
    "- Numeric: median impute + StandardScaler\n",
    "- Categorical: most-frequent impute + OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e66573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dab995",
   "metadata": {},
   "source": [
    "## 7) Logistic Regression + GridSearchCV\n",
    "\n",
    "**Scoring:** ROC-AUC (good for imbalanced classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Logistic Regression Grid Search\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "lr = LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "lr_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", lr)\n",
    "])\n",
    "\n",
    "lr_param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "    \"model__solver\": [\"lbfgs\"]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    estimator=lr_pipe,\n",
    "    param_grid=lr_param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print(\"Best LR params:\", lr_grid.best_params_)\n",
    "print(\"Best LR CV ROC-AUC:\", round(lr_grid.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f6498",
   "metadata": {},
   "source": [
    "## 8) XGBoost + GridSearchCV\n",
    "\n",
    "We tune a practical set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b12329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) XGBoost Grid Search\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", xgb)\n",
    "])\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"model__n_estimators\": [200, 400],\n",
    "    \"model__max_depth\": [3, 5],\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__subsample\": [0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator=xgb_pipe,\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(\"Best XGB params:\", xgb_grid.best_params_)\n",
    "print(\"Best XGB CV ROC-AUC:\", round(xgb_grid.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70f4b4",
   "metadata": {},
   "source": [
    "## 9) Held-out Test Evaluation + Saved Artifacts\n",
    "\n",
    "Saves classification reports, confusion matrices, ROC curves, and best params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Evaluation helpers\n",
    "def save_json(obj, path: Path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def export_classification_report(y_true, y_pred, out_csv: Path):\n",
    "    report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "    pd.DataFrame(report_dict).T.to_csv(out_csv, index=True)\n",
    "\n",
    "def plot_and_save_confusion_matrix(y_true, y_pred, title, outpath: Path):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_and_save_roc(y_true, y_proba, title, outpath: Path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc_val = roc_auc_score(y_true, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_val:.4f}\")\n",
    "    plt.plot([0,1], [0,1], \"k--\", label=\"Random\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=150)\n",
    "    plt.close()\n",
    "    return auc_val\n",
    "\n",
    "def evaluate_best_model(grid, model_name: str):\n",
    "    best_est = grid.best_estimator_\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    y_proba = best_est.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    test_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    save_json(grid.best_params_, OUTPUT_DIR / f\"best_params_{model_name}.json\")\n",
    "    export_classification_report(y_test, y_pred, OUTPUT_DIR / f\"classification_report_{model_name}.csv\")\n",
    "    plot_and_save_confusion_matrix(y_test, y_pred, f\"Confusion Matrix - {model_name}\", OUTPUT_DIR / f\"confusion_matrix_{model_name}.png\")\n",
    "    plot_and_save_roc(y_test, y_proba, f\"ROC Curve - {model_name}\", OUTPUT_DIR / f\"roc_{model_name}.png\")\n",
    "\n",
    "    return {\"Model\": model_name, \"Test ROC-AUC\": test_auc}\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_best_model(lr_grid, \"LogReg_GridSearch\"))\n",
    "results.append(evaluate_best_model(xgb_grid, \"XGB_GridSearch\"))\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Test ROC-AUC\", ascending=False)\n",
    "results_df.to_csv(OUTPUT_DIR / \"model_comparison_test_auc.csv\", index=False)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca1afa",
   "metadata": {},
   "source": [
    "## 10) XGBoost Feature Importance (Top 15)\n",
    "\n",
    "Extracts feature names after preprocessing (including one-hot expansion) and saves:\n",
    "- `feature_importance_XGB_top15.csv`\n",
    "- `feature_importance_XGB_top15.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Feature importance (XGB best)\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "pre = best_xgb.named_steps[\"preprocess\"]\n",
    "\n",
    "# Feature names\n",
    "num_features = numeric_cols\n",
    "cat_features = []\n",
    "if len(categorical_cols) > 0:\n",
    "    ohe = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_features = ohe.get_feature_names_out(categorical_cols).tolist()\n",
    "feature_names = num_features + cat_features\n",
    "\n",
    "xgb_model = best_xgb.named_steps[\"model\"]\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(15)\n",
    "feat_imp.to_csv(OUTPUT_DIR / \"feature_importance_XGB_top15.csv\", header=[\"importance\"])\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "sns.barplot(x=feat_imp.values, y=feat_imp.index)\n",
    "plt.title(\"XGBoost (Best) - Top 15 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"feature_importance_XGB_top15.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "feat_imp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d15406",
   "metadata": {},
   "source": [
    "## 11) Final Summary for README\n",
    "\n",
    "This cell prints CV AUC and test AUC. Use the saved artifacts in your README results section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26342c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Summary for README\n",
    "print(\"=== CV ROC-AUC (5-fold) ===\")\n",
    "print(\"LogReg best CV ROC-AUC:\", round(lr_grid.best_score_, 4))\n",
    "print(\"XGB   best CV ROC-AUC:\", round(xgb_grid.best_score_, 4))\n",
    "\n",
    "print(\"\\n=== Test ROC-AUC (held-out) ===\")\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\nArtifacts saved to:\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
