# Capstone Project: Model Comparison (GridSearchCV) - Logistic Regression vs XGBoost
# Target: readmitted_binary (or readmitted if your processed file uses that)

import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve
)

from xgboost import XGBClassifier

# -----------------------------
# 0) Paths & Output Folders
# -----------------------------
data_path = "../Processed Data/processed_diabetes_data.csv"
out_dir = "../output/readmitted_binary"
os.makedirs(out_dir, exist_ok=True)

# -----------------------------
# 1) Load Data
# -----------------------------
data = pd.read_csv(data_path)

# ---- IMPORTANT: choose correct target column ----
# If your processed file has 'readmitted_binary', use it.
# If it has 'readmitted', use it.
target_col = "readmitted_binary" if "readmitted_binary" in data.columns else "readmitted"

X = data.drop(columns=[target_col])
y = data[target_col].astype(int)  # ensure 0/1

print("Target column:", target_col)
print("Class distribution:\n", y.value_counts(normalize=True).round(3))

# -----------------------------
# 2) Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 5-fold stratified CV (required evidence for rubric)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# -----------------------------
# 3) GridSearchCV - Logistic Regression
# -----------------------------
lr_pipeline = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("model", LogisticRegression(max_iter=2000, solver="liblinear"))
])

lr_param_grid = {
    "model__C": [0.01, 0.1, 1, 10],
    "model__penalty": ["l1", "l2"]
}

lr_grid = GridSearchCV(
    estimator=lr_pipeline,
    param_grid=lr_param_grid,
    scoring="roc_auc",
    cv=cv,
    n_jobs=-1,
    verbose=0,
    return_train_score=True
)

lr_grid.fit(X_train, y_train)

# Save CV results
pd.DataFrame(lr_grid.cv_results_).to_csv(
    os.path.join(out_dir, "gridsearch_cv_results_LogReg.csv"),
    index=False
)

print("\n[LogReg] Best Params:", lr_grid.best_params_)
print("[LogReg] Best CV ROC-AUC:", round(lr_grid.best_score_, 4))

# Predict on test set
lr_best = lr_grid.best_estimator_
lr_proba = lr_best.predict_proba(X_test)[:, 1]
lr_pred = (lr_proba >= 0.5).astype(int)

lr_test_auc = roc_auc_score(y_test, lr_proba)
print("[LogReg] Test ROC-AUC:", round(lr_test_auc, 4))

# Save classification report
lr_report = pd.DataFrame(classification_report(y_test, lr_pred, output_dict=True)).T
lr_report.to_csv(os.path.join(out_dir, "classification_report_LogReg_gridsearch.csv"))

# Save confusion matrix
cm_lr = confusion_matrix(y_test, lr_pred)
plt.figure(figsize=(5,4))
plt.imshow(cm_lr)
plt.title("Confusion Matrix - Logistic Regression (GridSearch)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.xticks([0,1])
plt.yticks([0,1])
for (i, j), v in np.ndenumerate(cm_lr):
    plt.text(j, i, str(v), ha="center", va="center")
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "confusion_matrix_LogReg_gridsearch.png"), dpi=200)
plt.close()

# Save ROC curve
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_proba)
plt.figure(figsize=(6,5))
plt.plot(fpr_lr, tpr_lr, label=f"LogReg (AUC={lr_test_auc:.3f})")
plt.plot([0,1],[0,1],"--")
plt.title("ROC Curve - Logistic Regression (GridSearch)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "roc_LogReg_gridsearch.png"), dpi=200)
plt.close()

# -----------------------------
# 4) GridSearchCV - XGBoost
# -----------------------------
# Optional: handle imbalance via scale_pos_weight
pos = (y_train == 1).sum()
neg = (y_train == 0).sum()
scale_pos_weight = neg / pos if pos > 0 else 1

xgb_model = XGBClassifier(
    objective="binary:logistic",
    eval_metric="logloss",
    random_state=42,
    n_jobs=-1,
    scale_pos_weight=scale_pos_weight
)

xgb_param_grid = {
    "n_estimators": [200, 400],
    "max_depth": [3, 5],
    "learning_rate": [0.05, 0.1],
    "subsample": [0.8, 1.0],
    "colsample_bytree": [0.8, 1.0]
}

xgb_grid = GridSearchCV(
    estimator=xgb_model,
    param_grid=xgb_param_grid,
    scoring="roc_auc",
    cv=cv,
    n_jobs=-1,
    verbose=0,
    return_train_score=True
)

xgb_grid.fit(X_train, y_train)

pd.DataFrame(xgb_grid.cv_results_).to_csv(
    os.path.join(out_dir, "gridsearch_cv_results_XGB.csv"),
    index=False
)

print("\n[XGB] Best Params:", xgb_grid.best_params_)
print("[XGB] Best CV ROC-AUC:", round(xgb_grid.best_score_, 4))

xgb_best = xgb_grid.best_estimator_
xgb_proba = xgb_best.predict_proba(X_test)[:, 1]
xgb_pred = (xgb_proba >= 0.5).astype(int)

xgb_test_auc = roc_auc_score(y_test, xgb_proba)
print("[XGB] Test ROC-AUC:", round(xgb_test_auc, 4))

# Save classification report
xgb_report = pd.DataFrame(classification_report(y_test, xgb_pred, output_dict=True)).T
xgb_report.to_csv(os.path.join(out_dir, "classification_report_XGB_gridsearch.csv"))

# Save confusion matrix
cm_xgb = confusion_matrix(y_test, xgb_pred)
plt.figure(figsize=(5,4))
plt.imshow(cm_xgb)
plt.title("Confusion Matrix - XGBoost (GridSearch)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.xticks([0,1])
plt.yticks([0,1])
for (i, j), v in np.ndenumerate(cm_xgb):
    plt.text(j, i, str(v), ha="center", va="center")
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "confusion_matrix_XGB_gridsearch.png"), dpi=200)
plt.close()

# Save ROC curve
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_proba)
plt.figure(figsize=(6,5))
plt.plot(fpr_xgb, tpr_xgb, label=f"XGB (AUC={xgb_test_auc:.3f})")
plt.plot([0,1],[0,1],"--")
plt.title("ROC Curve - XGBoost (GridSearch)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "roc_XGB_gridsearch.png"), dpi=200)
plt.close()

# -----------------------------
# 5) Feature Importance (XGBoost)
# -----------------------------
feat_imp = pd.Series(xgb_best.feature_importances_, index=X.columns).sort_values(ascending=False)
feat_imp_top15 = feat_imp.head(15)

feat_imp_top15.to_csv(os.path.join(out_dir, "feature_importance_XGB_top15_gridsearch.csv"), header=["importance"])

plt.figure(figsize=(8,5))
feat_imp_top15.sort_values().plot(kind="barh")
plt.title("XGBoost Top 15 Feature Importances (GridSearch)")
plt.xlabel("Importance")
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "feature_importance_XGB_gridsearch.png"), dpi=200)
plt.close()

# -----------------------------
# 6) Summary Table (saved)
# -----------------------------
summary = pd.DataFrame({
    "Model": ["Logistic Regression (GridSearch)", "XGBoost (GridSearch)"],
    "Best CV ROC-AUC": [lr_grid.best_score_, xgb_grid.best_score_],
    "Test ROC-AUC": [lr_test_auc, xgb_test_auc]
}).round(4)

summary.to_csv(os.path.join(out_dir, "model_comparison_summary_gridsearch.csv"), index=False)

print("\nModel Comparison Summary (ROC-AUC):")
print(summary)
print(f"\nâœ… Saved all GridSearch artifacts to: {out_dir}")
